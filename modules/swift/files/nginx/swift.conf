server {
	listen 443 default_server deferred backlog=4096 reuseport ssl fastopen=150;
	listen [::]:443 default_server deferred backlog=4096 reuseport ssl fastopen=150;

	server_name static.wikitide.net swift-lb.wikitide.net;

	ssl_certificate /etc/ssl/localcerts/wikitide.net.crt;
	ssl_certificate_key /etc/ssl/private/wikitide.net.key;

	add_header Strict-Transport-Security "max-age=15768000; includeSubDomains; preload";

	location / {
		proxy_pass http://127.0.0.1;
		proxy_http_version 1.1;
		proxy_set_header Connection close; # should be default
		
		proxy_set_header Host $host;
		proxy_set_header X-Real-IP $remote_addr;
		proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

		proxy_read_timeout 180;

		proxy_redirect off;
		proxy_buffering off;
		proxy_request_buffering off;

		# Don't clobber the Server header from the backend.
		proxy_pass_header Server;

		# With proxy buffering off, proxy_buffer_size is still used to
		# define the singular chunk of buffer the content passes
		# through.  Apparently if nginx can't read all the response
		# headers in the first such chunk, it fails with 502.  Upping
		# this to 8k (from def 4k) solves it.
		# --
		# Raising this further from 8K to 32K
		# Note 32KB is also the similar default limit in our varnishes (http_resp_size)
		proxy_buffer_size       32k;
		proxy_buffers         4 32k;
	}
}
