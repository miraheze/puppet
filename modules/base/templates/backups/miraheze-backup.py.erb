#!/usr/bin/python3
# Backup script for Miraheze
# Created by John Lewis 2022

import argparse
import time
import tarfile
import os
import json
import glob
import requests

from fabric import Connection
from datetime import datetime

parser = argparse.ArgumentParser(description='Provides backup and download functionality for Miraheze backups from PCA.')
parser.add_argument('action', choices=['backup', 'download', 'find', 'unfreeze'], help='Action to be ran')
parser.add_argument('type', choices=['private', 'sslkeys', 'phabricator', 'sql', 'mediawiki-xml'], help='Type of backup to handle using the action')
parser.add_argument('--date', dest='date', help='Date for backup to download', metavar='YYYY-MM-DD')
parser.add_argument('--database', dest='database', help='Specific database to download or backup')
args = parser.parse_args()


def pca_connection():
    return Connection('gateways.storage.bhs.cloud.ovh.net', gateway='nc -6 -X connect -x bast.miraheze.org:8080 %h %p', user='pca', connect_kwargs={'password': '<%= @pca_password %>'})


def pca_web(method: str, url: str, expiry: int):
    pca_password = "<%= @pca_password %>"
    proxies = { 'https': 'http://bast.miraheze.org:8080' }
    json_data = { "auth": { "identity": { "methods": ["password"], "password": { "user": { "name": pca_password.split('.')[1], "domain": { "id": "default" }, "password": pca_password.split('.')[2] } } }, "scope": { "project": { "id": "76f9bc606a8044e08db7ebd118f6b19a", "domain": { "id": "default" } } } } }

    token = requests.post(f'https://auth.cloud.ovh.net/v3/auth/tokens', json=json_data, proxies=proxies, headers={ 'Content-Type': 'application/json' }).headers.get('X-Subject-Token')
    headers = { 'X-AUTH-TOKEN': token }
    
    if method == 'GET':
        return requests.get(url, headers=headers, proxies=proxies)
    elif method == 'POST':
        if expiry > 0:
            headers['X-Delete-After'] = f'{expiry*604800}'
        
        return requests.post(url, headers=headers, proxies=proxies)
    elif method == 'HEAD':
        return requests.head(url, headers=headers, proxies=proxies)
    else:
        raise Exception('Method is not currently implemented in Miraheze-Backup')


def backup_private(dt: str):
    tar = tarfile.open('private.tar.gz', 'w:gz')
    tar.add('/etc/puppetlabs/puppet/private', arcname='private')
    tar.close()

    pca_connection().put('private.tar.gz', f'private/{dt}.tar.gz', False)
    pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/private/{dt}.tar.gz', 4)

    os.remove('private.tar.gz')


def backup_sslkeys(dt: str):
    tar = tarfile.open('sslkeys.tar.gz', 'w:gz')
    tar.add('/etc/puppetlabs/puppet/ssl-keys', arcname='sslkeys')
    tar.close()

    pca_connection().put('sslkeys.tar.gz', f'sslkeys/{dt}.tar.gz', False)
    pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/sslkeys/{dt}.tar.gz', 4)

    os.remove('sslkeys.tar.gz')


def backup_phabricator(dt: str):
    os.system('/srv/phab/phabricator/bin/storage dump --compress --output backup.tar.gz')
    tar = tarfile.open('phabricator.tar.gz', 'w:gz')
    tar.add('backup.tar.gz', arcname='db')
    os.remove('backup.tar.gz')
    tar.add('/srv/phab/images', arcname='phabricator')
    tar.close()

    pca_connection().put('phabricator.tar.gz', f'phabricator/{dt}.tar.gz', False)
    pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/phabricator/{dt}.tar.gz', 4)

    os.remove('phabricator.tar.gz')


def backup_sql(dt: str, database: str):
    if database is None:
        os.system('/usr/bin/mydumper -N -W -k --less-locking -t 4 -c -x \'^(.*wiki(?!.*(objectcache|querycache|querycachetwo|recentchanges|searchindex)))\' --trx-consistency-only -o \'/srv/backups/dbs\'')
        dbs = [file for file in os.listdir('/srv/mariadb') if os.path.isdir(f'/srv/mariadb/{file}') and file[-4:] == 'wiki']
    else:
        os.system(f'/usr/bin/mysqldump -C --ignore-table={database}.objectcache --ignore-table={database}.querycache --ignore-table={database}.querycachetwo --ignore-table={database}.searchindex --ignore-table={database}.recentchanges {database} > /srv/backups/dbs/{database}.backup')
        dbs = [database]

    for db in dbs:
        tar = tarfile.open(f'{db}.tar.gz', 'w:gz')
        for dbfile in glob.glob(f'/srv/backups/dbs/{db}.*'):
            tar.add(dbfile, arcname=db)
            os.remove(dbfile)
        tar.close()

        pca_connection().put(f'{db}.tar.gz', f'sql/{db}/{dt}.tar.gz', False)
        pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/sql/{db}/{dt}.tar.gz', 5)

        os.remove(f'{db}.tar.gz')


def backup_mediawiki_xml(dt: str, database: str):
    if database is None:
        dbs = json.load(open('/srv/mediawiki/cache/databases.json'))['combi'].keys()
    else:
        dbs = [database]

    for db in dbs:
        os.system(f'/usr/bin/php /srv/mediawiki/w/maintenance/dumpBackup.php --logs --uploads --full --output="gzip:/srv/backups/{db}.xml.gz" --wiki {db}')

        pca_connection().put(f'/srv/backups/{db}.tar.gz', f'mediawiki-xml/{db}/{dt}.tar.gz', False)
        pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/mediawiki-xml/{db}/{dt}.tar.gz', 13)

        os.remove(f'/srv/backups/{db}.tar.gz')


def backup(source: str, database: str):
    dt = datetime.now().strftime('%Y-%m-%d')
    ts = time.time()
    print(f'Starting backup of \'{source}\' for date {dt}...')

    if source == 'private':
        backup_private(dt)
    elif source == 'sslkeys':
        backup_sslkeys(dt)
    elif source == 'phabricator':
        backup_phabricator(dt)
    elif source == 'sql':
        backup_sql(dt, database)
    elif source == 'mediawiki-xml':
        backup_mediawiki_xml(dt, database)

    print(f'Completed! This took {time.time() - ts}s')


def download_pca(file: str):
    check_header = pca_web('HEAD', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{file}', 0).headers.get('X-Ovh-Retrieval-State')
    
    if check_header == 'sealed':
        print(f'{file} is unable to be downloaded as the file is sealed. Please unfreeze the file using the unfreeze command.')
    elif check_header == 'unsealing':
        unseal_time = pca_web('HEAD', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{file}', 0).headers.get('X-Ovh-Retrieval-Delay')
        print(f'{file} is unable to be downloaded as the file is being unsealed. Please try again in {unseal_time}s.')
    else:
        try:
            pca_connection().get(file)
        except:
            print('Unable to download file! Please raise this on Phabricator if this is a repeat issue.')


def download(source: str, dt: str, database: str):
    ts = time.time()
    print(f'Downloading backup of \'{source}\' for date {dt}...')

    if source in ['private', 'sslkeys', 'phabricator']:
        download_pca(f'{source}/{dt}.tar.gz')
    elif source in ['sql', 'mediawiki-xml']:
        download_pca(f'{source}/{database}/{dt}.tar.gz')

    print(f'Completed! This took {time.time() - ts}s')


def find_backups(source: str, database: str):
    all_backups = pca_web('GET', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{source}', 0)
    backups_list = list(all_backups.text.split("\n"))
    
    if source in ['database', 'mediawiki-xml']:
        for backup_item in backups_list:
            if backup_item.split('/')[0] == database:
                print(backup_item.split('/')[1].split('.')[0])
    else:
        for backup_item in backups_list:
            print(backup_item)


def unfreeze_backup(source: str, dt: str, database: str):
    if source in ['private', 'sslkeys', 'phabricator']:
        file = f'{source}/{dt}.tar.gz'
    elif source in ['sql', 'mediawiki-xml']:
        file = f'{source}/{database}/{dt}.tar.gz'

    pca_web('GET', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{file}', 0)
    available_in = pca_web('HEAD', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{file}', 0).headers.get('X-Ovh-Retrieval-Delay')
    print(f'{file} has been unfrozen. It will be available to download in {int(available_in)/60} minutes.')

if __name__ == '__main__':

    if args.action == 'backup':
        backup(args.type, args.database)
    elif args.action == 'download':
        if not args.date:
            parser.exit(1, '--date is required when downloading a file!')

        download(args.type, args.date, args.database)
    elif args.action == 'find':
        find_backups(args.type, args.database)
    elif args.action == 'unfreeze':
        if not args.date:
            parser.exit(1, '--date is required when unfreezing a file!')

        unfreeze_backup(args.type, args.date, args.database)
