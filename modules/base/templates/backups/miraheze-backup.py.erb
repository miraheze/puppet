#!/usr/bin/python3
# Backup script for Miraheze
# Created by John Lewis 2022

import argparse
import glob
import json
import os
import requests
import subprocess
import tarfile
import time

from fabric import Connection
from datetime import datetime

parser = argparse.ArgumentParser(description='Provides backup and download functionality for Miraheze backups from PCA.')
parser.add_argument('action', choices=['backup', 'download', 'find', 'unfreeze'], help='Action to be ran')
parser.add_argument('type', choices=['private', 'sslkeys', 'phorge', 'sql', 'mediawiki-xml', 'swift-account-container', 'grafana'], help='Type of backup to handle using the action')
parser.add_argument('--date', dest='date', help='Date for backup to download', metavar='YYYY-MM-DD')
parser.add_argument('--database', dest='database', help='Specific database to download or backup')
args = parser.parse_args()


def pca_connection(status, *args):
    with Connection('gateways.storage.bhs.cloud.ovh.net', gateway='nc -6 -X connect -x bastion.wikitide.net:8080 %h %p', user='pca', connect_kwargs={'password': '<%= @pca_password %>'}) as c:
        if status == 'GET':
            c.get(*args)
        else:
            try:
                c.put(*args)
            except IOError:
                print(f'WARNING: encountered IOError uploading {args[0]} to {args[1]}') 


def pca_web(method: str, url: str, expiry: int):
    pca_password = "<%= @pca_password %>"
    proxies = { 'https': 'http://bastion.wikitide.net:8080' }
    json_data = { "auth": { "identity": { "methods": ["password"], "password": { "user": { "name": pca_password.split('.')[1], "domain": { "id": "default" }, "password": pca_password.split('.')[2] } } }, "scope": { "project": { "id": "76f9bc606a8044e08db7ebd118f6b19a", "domain": { "id": "default" } } } } }

    token = requests.post(f'https://auth.cloud.ovh.net/v3/auth/tokens', json=json_data, proxies=proxies, headers={ 'Content-Type': 'application/json' }).headers.get('X-Subject-Token')
    headers = { 'X-AUTH-TOKEN': token }
    
    if method == 'GET':
        return requests.get(url, headers=headers, proxies=proxies)
    elif method == 'POST':
        if expiry > 0:
            headers['X-Delete-After'] = f'{expiry*604800}'
        
        return requests.post(url, headers=headers, proxies=proxies)
    elif method == 'HEAD':
        return requests.head(url, headers=headers, proxies=proxies)
    else:
        raise Exception('Method is not currently implemented in Miraheze-Backup')


def backup_private(dt: str):
    tar = tarfile.open('private.tar.gz', 'w:gz')
    tar.add('/etc/puppetlabs/puppet/private', arcname='private')
    tar.close()

    pca_connection('PUT', 'private.tar.gz', f'private/{dt}.tar.gz', False)

    subprocess.call(f'rm -f private.tar.gz', shell=True)

    pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/private/{dt}.tar.gz', 4)
    segments_expire(f'private', f'{dt}.tar.gz', 4)


def backup_sslkeys(dt: str):
    tar = tarfile.open('sslkeys.tar.gz', 'w:gz')
    tar.add('/etc/puppetlabs/puppet/ssl-keys', arcname='sslkeys')
    tar.close()

    pca_connection('PUT', 'sslkeys.tar.gz', f'sslkeys/{dt}.tar.gz', False)

    subprocess.call(f'rm -f sslkeys.tar.gz', shell=True)

    pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/sslkeys/{dt}.tar.gz', 4)
    segments_expire(f'sslkeys', f'{dt}.tar.gz', 4)


def backup_swift_account_container(dt: str):
    tar = tarfile.open('swift_account_container.tar.gz', 'w:gz')
    tar.add('/srv/node', arcname='swift_account_container')
    tar.close()

    pca_connection('PUT', 'swift_account_container.tar.gz', f'swift-account-container/{dt}.tar.gz', False)

    subprocess.call(f'rm -f swift_account_container.tar.gz', shell=True)

    pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/swift-account-container/{dt}.tar.gz', 4)
    segments_expire(f'swift-account-container', f'{dt}.tar.gz', 4)


def backup_phorge(dt: str):
    subprocess.check_call(f'/srv/phorge/phorge/bin/storage dump --compress --output backup.tar.gz', shell=True)
    tar = tarfile.open('phorge.tar.gz', 'w:gz')
    tar.add('backup.tar.gz', arcname='db')
    subprocess.call(f'rm -f backup.tar.gz', shell=True)
    tar.add('/srv/phorge/images', arcname='phorge')
    tar.close()

    pca_connection('PUT', 'phorge.tar.gz', f'phorge/{dt}.tar.gz', False)

    subprocess.call(f'rm -f phorge.tar.gz', shell=True)

    pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/phorge/{dt}.tar.gz', 4)
    segments_expire(f'phorge', f'{dt}.tar.gz', 4)


def backup_sql(dt: str, database: str):
    if database is None:
        dbs = [file for file in sorted(os.listdir('/srv/mariadb')) if os.path.isdir(f'/srv/mariadb/{file}') and file[-4:] == 'wiki']
    else:
        dbs = [database]

    if not os.path.isdir(f'/srv/backups/dbs'):
        os.makedirs(f'/srv/backups/dbs')

    for db in dbs:
        print(f'Backing up database \'{db}\'...')

        subprocess.check_call(f'/usr/bin/ionice -c2 -n7 /usr/bin/mysqldump --quick --skip-lock-tables --single-transaction -C --ignore-table={db}.objectcache --ignore-table={db}.querycache --ignore-table={db}.querycachetwo --ignore-table={db}.recentchanges --ignore-table={db}.searchindex {db} | /usr/bin/nice /usr/bin/pigz -p <%= @facts['processors']['count']/8 > 1 ? @facts['processors']['count']/8 : 1 %> > /srv/backups/dbs/{db}.sql.gz', shell=True)
        pca_connection('PUT', f'/srv/backups/dbs/{db}.sql.gz', f'sql/{db}/{dt}.sql.gz', False)

        subprocess.check_call(f'rm -f /srv/backups/dbs/{db}.sql.gz', shell=True)

        pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/sql/{db}/{dt}.sql.gz', 5)
        segments_expire(f'sql', f'{db}/{dt}.sql.gz', 5)


def backup_grafana_db(dt: str):
    subprocess.check_call(f'/usr/bin/sqlite3 /var/lib/grafana/grafana.db ".backup \'/var/lib/grafana/grafana_backup.db\'"', shell=True)
    tar = tarfile.open('grafana.tar.gz', 'w:gz')
    tar.add('/var/lib/grafana/grafana_backup.db', arcname='grafana_backup.db')
    tar.close()
    
    pca_connection('PUT', 'grafana.tar.gz', f'sql/grafana/{dt}.tar.gz', False)

    subprocess.call(f'rm -f grafana.tar.gz /var/lib/grafana/grafana_backup.db', shell=True)

    pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/sql/grafana/{dt}.tar.gz', 4)
    segments_expire(f'sql', f'grafana/{dt}.tar.gz', 4)


def backup_mediawiki_xml(dt: str, database: str):
    if database is None:
        dbs = json.load(open('/srv/mediawiki/cache/databases.json'))['combi'].keys()
    else:
        dbs = [database]

    for db in dbs:
        try:
            subprocess.check_call(f'/usr/bin/php /srv/mediawiki/w/maintenance/dumpBackup.php --logs --uploads --full --output="gzip:/srv/backups/{db}.xml.gz" --wiki {db}', shell=True)
        except subprocess.CalledProcessError as e:
            print(f'WARNING: encountered CalledProcessError backing up {db} with returned code {e.returncode} and output:\n')
            print(f'{e.output}\n')
            subprocess.call(f'rm -f /srv/backups/{db}.xml.gz', shell=True)
            continue

        pca_connection('PUT', f'/srv/backups/{db}.xml.gz', f'mediawiki-xml/{db}/{dt}.xml.gz', False)

        subprocess.call(f'rm -f /srv/backups/{db}.xml.gz', shell=True)

        pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/mediawiki-xml/{db}/{dt}.xml.gz', 13)
        segments_expire(f'mediawiki-xml', f'{db}/{dt}.xml.gz', 13)


def segments_expire(source: str, object: str, expire: int):
    """We have to apply the X-Delete-After header to segments as well if they exist for a object.
    Doing it just for a main object doesn't apply it to all the segments of the object.
    """
    segments = pca_web('GET', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{source}_segments', 0)
    segments_list = list(segments.text.split("\n"))

    for segment in segments_list:
            if segment.startswith(f'{object}/'):
                pca_web('POST', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{source}_segments/{segment}', expire)


def backup(source: str, database: str):
    dt = datetime.now().strftime('%Y-%m-%d')
    ts = time.time()
    print(f'Starting backup of \'{source}\' for date {dt}...')

    if source == 'private':
        backup_private(dt)
    elif source == 'sslkeys':
        backup_sslkeys(dt)
    elif source == 'swift-account-container':
        backup_swift_account_container(dt)
    elif source == 'phorge':
        backup_phorge(dt)
    elif source == 'sql':
        backup_sql(dt, database)
    elif source == 'grafana':
        backup_grafana_db(dt)
    elif source == 'mediawiki-xml':
        backup_mediawiki_xml(dt, database)

    print(f'Completed! This took {time.time() - ts}s')


def download_pca(file: str):
    check_header = pca_web('HEAD', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{file}', 0).headers.get('X-Ovh-Retrieval-State')
    
    if check_header == 'sealed':
        print(f'{file} is unable to be downloaded as the file is sealed. Please unfreeze the file using the unfreeze command.')
    elif check_header == 'unsealing':
        unseal_time = pca_web('HEAD', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{file}', 0).headers.get('X-Ovh-Retrieval-Delay')
        print(f'{file} is unable to be downloaded as the file is being unsealed. Please try again in {unseal_time}s.')
    else:
        try:
            pca_connection('GET', file)
        except:
            print('Unable to download file! Please raise this on Phorge if this is a repeat issue.')


def download(source: str, dt: str, database: str):
    ts = time.time()
    print(f'Downloading backup of \'{source}\' for date {dt}...')

    if source in ['private', 'sslkeys', 'phorge', 'swift-account-container']:
        download_pca(f'{source}/{dt}.tar.gz')
    elif source in ['mediawiki-xml']:
        download_pca(f'{source}/{database}/{dt}.xml.gz')
    elif source in ['grafana']:
        download_pca(f'sql/{source}/{dt}.tar.gz')
    elif source in ['sql']:
        download_pca(f'{source}/{database}/{dt}.sql.gz')

    print(f'Completed! This took {time.time() - ts}s')


def find_backups(source: str, database: str):
    all_backups = pca_web('GET', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{source}', 0)
    backups_list = list(all_backups.text.split("\n"))
    
    if source in ['database', 'mediawiki-xml']:
        for backup_item in backups_list:
            if backup_item.split('/')[0] == database:
                print(backup_item.split('/')[1].split('.')[0])
    else:
        for backup_item in backups_list:
            print(backup_item)


def unfreeze_backup(source: str, dt: str, database: str):
    if source in ['private', 'sslkeys', 'phorge', 'swift-account-container']:
        file = f'{source}/{dt}.tar.gz'
    elif source in ['grafana']:
        file = f'sql/{source}/{dt}.tar.gz'
    elif source in ['mediawiki-xml']:
        file = f'{source}/{database}/{dt}.xml.gz'
    elif source in ['sql']:
        file = f'{source}/{database}/{dt}.sql.gz'

    pca_web('GET', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{file}', 0)
    available_in = pca_web('HEAD', f'https://storage.bhs.cloud.ovh.net/v1/AUTH_76f9bc606a8044e08db7ebd118f6b19a/{file}', 0).headers.get('X-Ovh-Retrieval-Delay')
    print(f'{file} has been unfrozen. It will be available to download in {int(available_in)/60} minutes.')

if __name__ == '__main__':

    if args.action == 'backup':
        backup(args.type, args.database)
    elif args.action == 'download':
        if not args.date:
            parser.exit(1, '--date is required when downloading a file!')

        download(args.type, args.date, args.database)
    elif args.action == 'find':
        find_backups(args.type, args.database)
    elif args.action == 'unfreeze':
        if not args.date:
            parser.exit(1, '--date is required when unfreezing a file!')

        unfreeze_backup(args.type, args.date, args.database)
