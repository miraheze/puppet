# Example config file for running an eventgate instance.
# This (optionally) uses a stream config file to restrict schemas
# in streams.
# Number of worker processes to spawn.
# Set to 0 to run everything in a single process without clustering.
# Use 'ncpu' to run as many workers as there are CPU units
num_workers: 0
# Log error messages and gracefully restart a worker if v8 reports that it
# uses more heap (note: not RSS) than this many mb.
worker_heap_limit_mb: 250
# Logger info
logging:
  level: debug
#  streams:
#  # Use gelf-stream -> logstash
#  - type: gelf
#    host: logstash1003.eqiad.wmnet
#    port: 12201
# Statsd metrics reporter
metrics:
#  - type: prometheus
#    port: 9102
  - type: statsd
    host: 10.0.15.145
    port: 8125
services:
  - name: EventGate
    # a relative path or the name of an npm package, if different from name
    module: ./src/app.js
    # optionally, a version constraint of the npm package
    # version: ^0.4.0
    # per-service config
    conf:
      port: 8192
      # uncomment to only listen on localhost
      #interface: localhost
      # Events can be large; increase max body size
      max_body_size: 4mb

      # Set cors to false by default.
      cors: false

      # more per-service config settings
      user_agent: eventgate

      # EventGate will be instantiated from the factory method returned by this module.
      eventgate_factory_module: '/srv/eventgate/eventgate-custom.js'

      # This field in each event will be used to extract a
      # (possibly relative) schema uri.  The default is $schema.
      # An array of field names will cause EventGate to search for
      # You can alteratively use a remote HTTP base URI.
      # fields by these names in each event, using the first match.
      schema_uri_field: $schema

      # If set, these URIs will be prepended to any relative schema URI
      # extracted from each event's schema_field.  The resulting URLs will
      # be searched until a schema is found.  Change this
      # to match paths to your schema repositories.
      schema_base_uris: [
          https://schema.wikimedia.org/repositories/primary/jsonschema,
          https://schema.wikimedia.org/repositories/secondary/jsonschema
      ]

      # These schema URIs will be 'precached' on service startup.
      # They should be resolveable by the URI prefixes in schema_base_uris.
      schema_precache_uris:
        - /error/1.0.0
        - /test/event/1.0.0

      # If set, this will be appended to every extracted schema_uri if that schema_uri
      # does not already end with a file extension.
      #schema_file_extension: .yaml

      # If this is true, then schema_uris in events will be allowed to
      # point at any URL domain.  If false, they must only contain URI paths,
      # which will be prefixed with one of the schema_base_uris.
      # Setting this to false restricts schema URLs to those specified
      # in schema_base_uris.  This defaults to false.
      allow_absolute_schema_uris: false

      # This field in each event will be used to extract a destination 'stream' name.
      # This will equal the destination Kafka topic, unless a topic prefix
      # is also configured.
      stream_field: meta.stream
      topic_prefix: 'default.'
  
      # This field will be used in log messages to uniquely ID each event.
      id_field: meta.id

      # This field will be used to extract and set a Kafka message timestamp.
      dt_field: meta.dt

      # A map of header names to field names.  On an incoming event, if the header
      # name is present, and if the field name is present in that event's schema
      # (or if the immediate parent of that field is present and has map type),
      # that field in the event will be set to the value of the corresponding header.
      http_request_headers_to_fields:
        x-request-id: meta.request_id
        user-agent: http.request_headers.user-agent

      # If a validation error is encountered, a validation error event
      # will be produced to this stream.
      error_stream: eventgate.error.validation

      # A URI from which 'stream configuration' will be fetched.  The result
      # should be a static stream name to stream config map.
      # The object returned by this URI should map a stream name to its config,
      # which must include schema_title.  schema_title must match exactly
      # incoming event schema's title field, or it will be rejected.
      #
      # If stream_config_uri is undefined, then any event will be allowed in
      # any stream, as long as it validates with its schema.
      #
      #stream_config_uri: stream-config.yaml

      # If provided, these options will be passed when opening stream_config_uri
      # for reading.  If this is a local file path, fs.readFile options should be
      # given.  If this is a remote http URI, preq.get options should be used.
      #stream_config_uri_options: {'headers': {'Host': 'streamconfig.service.org'}}

      # If set, the stream configs are expected to live in a subobject
      # of the result object returned from stream_config_uri. The stream
      # configs object will be extracted at this path.
      # This should be a path string that Lodash#get understands.
      #stream_config_object_path: 'streams'

      # How long in seconds stream configs live in cache before being expired.
      # 0 means no expiration. This has no affect if stream_config_is_dynamic: false
      #stream_config_ttl: 0

      # Retry up to this many times when fetching stream config from stream_config_uri.
      # Default: 1
      #stream_config_retries: 1

      # If test_events is set, a GET /v1/_test/events route will be added.
      # When this route is requested, these test_events will be processed through EventGate
      # as if it they were directly POSTed to /v1/events.
      # This is useful for readiness probes that want to make sure the service
      # is alive and ready to produce events end to end.
      test_events: [{$schema: /test/0.0.1, meta: {stream: test_events, id: "12345678-1234-5678-1234-567812345678"}, test: "example string"}]

      # If this is set and an event does not have schema_uri_field, the value of
      # event[schema_uri_field] will be set to the value of this HTTP query paramater
      schema_uri_query_param: schema_uri

      # If this is set and an event does not have stream_field, the value of
      # event[stream_field] will be set to the value of this HTTP query paramater.
      stream_query_param: stream

      # kafka configs go here.
      kafka:
        conf:
          metadata.broker.list: '10.0.18.146:9092'
          compression.codec: snappy
          message.max.bytes: 4194304
          # Silence noisy connection reaper logging
          # https://github.com/Blizzard/node-rdkafka/issues/326
          # https://github.com/edenhill/librdkafka/issues/768#issuecomment-246302856
          log.connection.close: false
          # report Kafka producer stats every 5 seconds
          statistics.interval.ms: 5000
          ## Uncomment the below to enable rdkafka trace logging
          # event_cb: true
          # log_level: 7
          # debug: broker,topic,msg
        # kafka topic conf goes here
        topic_conf: {}
        # Producer type specific overrides.
        # If you need to configure some producer specific settings,
        # e.g. different batch settings, you can provide them here.
        hasty:
          conf:
            # HastyProducer doesn't block HTTP clients, so we can
            # afford to wait for a largish batch size.
            queue.buffering.max.ms: 1000
            # Custom kafka config, will call producer.setPollInterval
            # with this value if set.
            producer.poll.interval.ms: 100
        guaranteed:
          conf:
            # GuaranteedProducer does block HTTP clients, so we attempt to send
            # the produce request as soon as possible, rather than waiting
            # for larger batches.
            queue.buffering.max.ms: 0
            # Custom kafka config, will call producer.setPollInterval
            # with this value if set.
            producer.poll.interval.ms: 10
