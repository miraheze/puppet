changeprop::jobqueue: true
changeprop::jobrunner_host: 'http://localhost:9007'
changeprop::jobrunner_high_timeout_host: 'http://localhost:9008'
changeprop::videoscaler_host: 'http://localhost:9009'
jobrunner_haproxy::backends:
  mwtask151:
    ip: 10.0.15.150
    port: 9006
  mwtask161:
    ip: 10.0.16.157
    port: 9006
  mwtask171:
    ip: 10.0.17.144
    port: 9006
  mwtask181:
    ip: 10.0.18.106
    port: 9006
changeprop::num_workers: 8
changeprop::semantic_mediawiki_concurrency: 6
changeprop::low_traffic_concurrency: 20
changeprop::high_traffic_jobs_config:
  ThumbnailRender:
    concurrency: 5
  categoryMembershipChange:
    concurrency: 10
  # CNDPurge is quite low-volume, but it uses delayed execution,
  # so avoid putting it together with other low-volume jobs so that it doesn't
  # block execution for others.
  cdnPurge:
    concurrency: 10
  # RecordLinks is normally low-volume, but could have big spikes
  # when maintenance scripts are run. Elevated concurrency
  RecordLintJob:
    concurrency: 10
    consumer_batch_size: 10
  wikibase-addUsagesForPage:
    concurrency: 4
  LocalGlobalUserPageCacheUpdateJob:
    # This job is prone to large spikes, so having it on the low_traffic_jobs queue
    # blocks other jobs.
    concurrency: 25
  notificationGetStartedJob:
    concurrency: 2
    # All the jobs of this kind are delayed exactly 48 hours, avoid reenqueueing them
    # by setting the reenqueue delay to 72 hours
    reenqueue_delay: 259200
  notificationKeepGoingJob:
    concurrency: 2
    # All the jobs of this kind are delayed exactly 48 hours, avoid reenqueueing them
    # by setting the reenqueue delay to 72 hours
    reenqueue_delay: 259200
  notificationReEngageJob:
    concurrency: 2
    # All the jobs of this kind are delayed exactly 48 hours, avoid reenqueueing them
    # by setting the reenqueue delay to 72 hours
    reenqueue_delay: 259200
  newcomerTasksCacheRefreshJob:
    concurrency: 2
    # All the jobs of this kind are delayed exactly 144 hours (6 days), avoid reenqueueing them
    # by setting the reenqueue delay to 7 days
    reenqueue_delay: 604800
  refreshUserImpactJob:
    concurrency: 3
  # For cirrus search jobs the retries are built into the job itself,
  # so disable the retries by change-prop. We need special rules for cirrus
  # jobs because they need special configuration.
  cirrusSearchCheckerJob:
    disable_delayed_execution: true
    retry_limit: 0
    concurrency: 10
  cirrusSearchDeleteArchive:
    retry_limit: 0
    concurrency: 5
  cirrusSearchDeletePages:
    retry_limit: 0
    concurrency: 5
  cirrusSearchIncomingLinkCount:
    retry_limit: 0
    concurrency: 10
  cirrusSearchLinksUpdate:
    retry_limit: 0
    concurrency: 10
  cirrusSearchLinksUpdatePrioritized:
    retry_limit: 0
    concurrency: 6
  cirrusSearchOtherIndex:
    retry_limit: 0
    concurrency: 5
  wikibase-InjectRCRecords:
    concurrency: 2
  parsoidCachePrewarm:
    concurrency: 20
  htmlCacheUpdate:
    concurrency: 20
  refreshLinks:
    concurrency: 10
    # Abandon jobs which root job is more than 1 week long
    root_claim_ttl: 604800
  smw.changePropagationClassUpdate:
    concurrency: 15
  smw.changePropagationDispatch:
    concurrency: 15
  smw.changePropagationUpdate:
    concurrency: 15
  # Translation jobs tend to be low traffic but are being delayed when other
  # low traffic jobs have a large spike. It is being moved to its own queue to
  # improve editing experience for users
  UpdateTranslatablePageJob:
    concurrency: 2
  RenderTranslationPageJob:
    concurrency: 2
  MWScriptJob:
    concurrency: 2

  # These jobs need to be ran with priority so are using their own queues
  LocalRenameUserJob:
    disable_delayed_execution: true
    concurrency: 1
  SetContainersAccessJob:
    disable_delayed_execution: true
    concurrency: 2
  CreateWikiJob:
    disable_delayed_execution: true
    concurrency: 1
  ImportDumpNotifyJob:
    disable_delayed_execution: true
    concurrency: 2
  RottenLinksJob:
    concurrency: 6
  GlobalNewFilesDeleteJob:
    concurrency: 2
  GlobalNewFilesInsertJob:
    concurrency: 2
  GlobalNewFilesMoveJob:
    concurrency: 2

changeprop::high_traffic_high_timeout_jobs_config:
  cirrusSearchElasticaWrite:
    retry_limit: 0
    concurrency: 10
    reenqueue_delay: 3600
    timeout: 600000
  NamespaceMigrationJob:
    disable_delayed_execution: true
    concurrency: 1
    timeout: 9000000
  DataDumpGenerateJob:
    concurrency: 1
    retry_limit: 0
    timeout: 86400000
  ImportDumpJob:
    disable_delayed_execution: true
    retry_limit: 0
    concurrency: 1
    timeout: 259200000
  RemovePIIJob:
    disable_delayed_execution: true
    concurrency: 1
    timeout: 86400000
  RequestWikiAIJob:
    concurrency: 1
    timeout: 86400000

changeprop::videoscaler_jobs_config:
  webVideoTranscode:
    timeout: 86400000
    concurrency: 1
    retry_limit: 1
    consumer:
      max.poll.interval.ms: 7200000  # 2h
  webVideoTranscodePrioritized:
    concurrency: 1
    timeout: 86400000
    retry_limit: 1
    consumer:
      max.poll.interval.ms: 7200000  # 2h

changeprop::latency_sensitive_jobs_config:
  # AssembleUploadChunks, PublishStashedFile, and UploadFromUrl,
  # are considered latency sensitive, as high backlog time presents a poor
  # user experience for async uploads.
  AssembleUploadChunks:
    concurrency: 5
  PublishStashedFile:
    concurrency: 5
  UploadFromUrl:
    concurrency: 5

changeprop::semantic_mediawiki_jobs:
  - SMWRefreshJob
  - SMWUpdateJob
#  - smw.changePropagationClassUpdate
#  - smw.changePropagationDispatch
#  - smw.changePropagationUpdate
  - smw.deferredConstraintCheckUpdateJob
  - smw.elasticFileIngest
  - smw.elasticIndexerRecovery
  - smw.entityIdDisposer
  - smw.fulltextSearchTableRebuild
  - smw.fulltextSearchTableUpdate
  - smw.parserCachePurgeJob
  - smw.propertyStatisticsRebuild
  - smw.refresh
  - smw.update
  - smw.updateDispatcher

prometheus::exporter::statsd_exporter::use_defaults: false
role::prometheus::statsd_exporter::mappings:
  - match: '*.gc.*'
    name: service_runner_gc_microseconds
    timer_type: histogram
    buckets: [ 5e+2, 1e+3, 5e+3, 10e+3, 15e+3, 30e+3, 100e+3 ]
    labels:
      service: $1
      event: $2

  - match: '*.heap.*'
    # service-runner abuses timer for heap data
    name: service_runner_${2}_heap_kilobytes
    timer_type: histogram
    buckets: [1e+3, 1e+4, 1e+5, 1e+6, 1e+7]
    labels:
      service: $1

  - match: '(.+)\.internal-startup\.(.+)-retry_exec'
    match_type: regex
    name: "${1}_retry_rule_processing"
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.(.+)_exec'
    match_type: regex
    name: "${1}_normal_rule_processing"
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.(.+)-retry_delay'
    match_type: regex
    name: "${1}_retry_rule_processing_delay"
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.(.+)_delay'
    match_type: regex
    name: "${1}_normal_rule_processing_delay"
    timer_type: histogram
    buckets: [1, 30, 60, 300, 600, 1800, 3600, 21600, 86400]  # 1s, 30s, 1m, 5m, 10m, 30m, 1h, 6h, 1d
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.(.+)_totaldelay'
    match_type: regex
    name: "${1}_normal_rule_processing_total_delay"
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.(.+)_dequeue'
    match_type: regex
    name: "${1}_dequeue"
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.(.+)_dedupe'
    match_type: regex
    name: "${1}_dedupe"
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.(.+)_blacklist'
    match_type: regex
    name: "${1}_blacklist"
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.(.+)_abandon'
    match_type: regex
    name: "${1}_abandon"
    labels:
      service: $1
      rule: $2

  - match: '(.+)\.internal-startup\.produce_(.+)\.(.+)'
    match_type: regex
    name: "${1}_produce"
    labels:
      service: $1
      stream: $2
      partition: $3

  - match: '(.+)\.internal-startup\.ratelimit_(.+)_(.+)'
    match_type: regex
    name: "${1}_ratelimit_timing"
    labels:
      service: $1
      function: $2
      timing: $3

http_proxy: 'http://bastion.fsslc.wtnet:8080'

redis::heap: '5GB'
redis::maxmemory_policy: volatile-lru
